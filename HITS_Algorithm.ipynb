{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "JTj03wGlnnD9",
   "metadata": {
    "id": "JTj03wGlnnD9"
   },
   "source": [
    "# 1. Collecting data set and Importing necessary libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VLZ8t-vCn7VP",
   "metadata": {
    "id": "VLZ8t-vCn7VP"
   },
   "source": [
    "#### 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "76b6805a",
   "metadata": {
    "id": "76b6805a"
   },
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "rXn4RjIiNikm",
   "metadata": {
    "id": "rXn4RjIiNikm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WriA7y8Sn_xO",
   "metadata": {
    "id": "WriA7y8Sn_xO"
   },
   "source": [
    "#### 1.2 Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cd4ff13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the web graph\n",
    "G = nx.read_gpickle(\"web_graph.gpickle\")\n",
    "adj = nx.to_numpy_array(G)\n",
    "adj_tran = adj.T\n",
    "np.array(list(G.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3bc45e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Business: Mills Grabs \\\\$1B Portfolio; Taubman Likely to Lose Contracts Mills Corp. agreed to purchase a 50 percent interest in nine malls owned by General Motors Asset Management Corp. for just over \\\\$1 billion, creating a new joint venture between the groups. The deal will extend ...'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading input text files\n",
    "text = []\n",
    "for x in range(len(G.nodes)):\n",
    "    text.append(G.nodes[x]['page_content'])\n",
    "text[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02Pj8YTNoE2S",
   "metadata": {
    "id": "02Pj8YTNoE2S"
   },
   "source": [
    "# 2. Removal of punctuation and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "76fcb297",
   "metadata": {
    "id": "76fcb297"
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "# tokenise the document\n",
    "def tokenize(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    token_words= [word for word in words if word.isalnum()]     #takes only the charecters which are either numbers or alphabets\n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8b284948",
   "metadata": {
    "id": "8b284948"
   },
   "outputs": [],
   "source": [
    "# remove stop words from tokens\n",
    "stopwords = stopwords.words('english')\n",
    "def stopwords_clr(sentence):\n",
    "    tokens_clr= [token for token in sentence if token.lower() not in stopwords] #takes only the words which are not in stopwords\n",
    "    return tokens_clr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4v98f8ioUht",
   "metadata": {
    "id": "p4v98f8ioUht"
   },
   "source": [
    "# 3. Normalization using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "051bb44b",
   "metadata": {
    "id": "051bb44b"
   },
   "outputs": [],
   "source": [
    "#stemming the words to root form\n",
    "stem = PorterStemmer()\n",
    "\n",
    "def stem_tokens(sentence):\n",
    "    tokens_stem = []\n",
    "    for token in sentence:\n",
    "        tokens_stem.append(stem.stem(token))     #stems the token and appends into tokens_stem list\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DYnu1zvHoq-n",
   "metadata": {
    "id": "DYnu1zvHoq-n"
   },
   "source": [
    "# 4. Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "-V_jGbRWHGqh",
   "metadata": {
    "id": "-V_jGbRWHGqh"
   },
   "outputs": [],
   "source": [
    "#tokenizes 'cont', removes stopwords and stems 'cont' \n",
    "def preprocess(cont):\n",
    "    return \" \".join(stopwords_clr(stem_tokens(stopwords_clr(tokenize(cont)))))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "L553rLkxHGqh",
   "metadata": {
    "id": "L553rLkxHGqh"
   },
   "outputs": [],
   "source": [
    "processed_data = []    #This contains the pre-processed data of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "BEaCdwWYHGqh",
   "metadata": {
    "id": "BEaCdwWYHGqh"
   },
   "outputs": [],
   "source": [
    "for i in range(len(text)):                       #goes through all the documents in text list\n",
    "  processed_data.append(preprocess(text[i]))     #appends the preprocessed document to preprocessed_data list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feba98c",
   "metadata": {
    "id": "3feba98c"
   },
   "source": [
    "# 5. Construct inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d6e5a308",
   "metadata": {
    "id": "d6e5a308"
   },
   "outputs": [],
   "source": [
    "inv_index = {}      #creating inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0d60460f",
   "metadata": {
    "id": "0d60460f"
   },
   "outputs": [],
   "source": [
    "#Indexing the inputted document\n",
    "def indexing(document, index):\n",
    "    words = nltk.word_tokenize(document)          #tokenizes the document\n",
    "    for word in words:                          \n",
    "        if(inv_index.get(word) is None):          #check whether word is there in inv_index or not\n",
    "            inv_index[word] = [index]               \n",
    "        elif not index in inv_index.get(word):     \n",
    "            inv_index.get(word).append(index)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c9d9be9a",
   "metadata": {
    "id": "c9d9be9a"
   },
   "outputs": [],
   "source": [
    "for x in range(len(processed_data)):\n",
    "    indexing(processed_data[x], x)          #indexing the preprocessd data of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "SNjSGfnEOsXN",
   "metadata": {
    "id": "SNjSGfnEOsXN"
   },
   "outputs": [],
   "source": [
    "keys = list(inv_index.keys())       #Keys contains a list of all terms in the dictionary\n",
    "postings = list(inv_index.values())    #Postings contain the posting list of all terms in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Qvtu23B2Eul",
   "metadata": {
    "id": "5Qvtu23B2Eul"
   },
   "source": [
    "# 7. Boolean query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "38d65be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_posting(term, inv_index):\n",
    "    if term not in inv_index.keys():\n",
    "        posting = []\n",
    "    else:\n",
    "        posting = inv_index[term]\n",
    "    return posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "pDwBCqAH2FUy",
   "metadata": {
    "id": "pDwBCqAH2FUy"
   },
   "outputs": [],
   "source": [
    "#Processing a boolean query and finding the appropriate documents\n",
    "def boolean_query(query, inv_index):\n",
    "  terms = query.split(' ')\n",
    "  bool_words = []\n",
    "  diff_words = []\n",
    "\n",
    "  stem = PorterStemmer()\n",
    "\n",
    "  for term in terms:\n",
    "    if term.lower() != 'and' and term.lower() != 'or' and term.lower() != 'not':\n",
    "      diff_words.append(stem.stem(term))\n",
    "    else:\n",
    "      bool_words.append(term)\n",
    "  \n",
    "  #print(bool_words, diff_words)\n",
    "  \n",
    "  posting_term = []\n",
    "  posting_comb = []\n",
    "\n",
    "  for term in diff_words:\n",
    "    posting_term = gen_posting(term, inv_index)\n",
    "    posting_comb.append(posting_term)\n",
    "\n",
    "  #print(posting_comb)\n",
    "\n",
    "\n",
    "  i = 0\n",
    "  x = 0\n",
    "  z = len(bool_words)\n",
    "    \n",
    "  while i < z:\n",
    "    \n",
    "    if bool_words[x] == 'not':\n",
    "      all_docs = set(list(range(len(processed_data))))\n",
    "      res = list(all_docs - set(posting_comb[x]))\n",
    "      posting_comb.remove(posting_comb[x])\n",
    "      posting_comb.insert(x, res)\n",
    "      bool_words.remove(bool_words[x])\n",
    "      i = i + 1\n",
    "    \n",
    "    elif bool_words[x] == 'and':\n",
    "        if (x + 1) < len(bool_words) and bool_words[x + 1] == 'not':\n",
    "            all_docs = set(list(range(len(processed_data))))\n",
    "            res = list(all_docs - set(posting_comb[x + 1]))\n",
    "            bool_words.remove(bool_words[x + 1])\n",
    "            i = i + 1\n",
    "        else:\n",
    "            res = posting_comb[x + 1]\n",
    "        intersection = list(set(posting_comb[x]).intersection(res))\n",
    "        posting_comb.remove(posting_comb[x])\n",
    "        posting_comb.remove(posting_comb[x])\n",
    "        posting_comb.insert(x, intersection)\n",
    "        bool_words.remove(bool_words[x])\n",
    "        i = i + 1\n",
    "        \n",
    "    elif bool_words[x] == 'or':\n",
    "        x = x + 1\n",
    "        i = i + 1\n",
    "        \n",
    "  #print(posting_comb)\n",
    "  #print(bool_words)\n",
    "    \n",
    "  i = 0      \n",
    "  while i < len(bool_words):\n",
    "    union = posting_comb[0] + list(set(posting_comb[1]) - set(posting_comb[0]))\n",
    "    #print(union)\n",
    "    posting_comb.remove(posting_comb[0])\n",
    "    posting_comb.remove(posting_comb[0])\n",
    "    posting_comb.insert(0, union)\n",
    "    i = i + 1\n",
    "         \n",
    "      \n",
    "  #print(posting_comb)\n",
    "  return posting_comb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "42ff4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_query(string):\n",
    "    inp_query = ''\n",
    "    \n",
    "    lst = stopwords_clr(tokenize(string))\n",
    "\n",
    "    index = 0\n",
    "    for x in range(2 * len(lst)):\n",
    "        if x % 2 == 0:\n",
    "            inp_query += lst[index]\n",
    "            index += 1\n",
    "        elif x < 2 * len(lst) - 1:\n",
    "            inp_query += ' and '\n",
    "            \n",
    "    return inp_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "WYIgVBpI9Lb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYIgVBpI9Lb6",
    "outputId": "7830cfa5-ab30-4678-cf1e-286d0aea8094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gunfire\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gunfire'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = input()\n",
    "\n",
    "inp_query = gen_query(string)\n",
    "\n",
    "inp_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "617774b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_doc = []\n",
    "out_doc = boolean_query(inp_query, inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490d094",
   "metadata": {},
   "source": [
    "# 8. Generating Base Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ae1b5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_root_bin(out_doc, G):\n",
    "    root = np.array(out_doc)\n",
    "    root_bin = []\n",
    "    for x in range(len(G.nodes)):\n",
    "        if x in root:\n",
    "            root_bin.append(1)\n",
    "        else:\n",
    "            root_bin.append(0)\n",
    "            \n",
    "    root_bin = np.array(root_bin)\n",
    "    return root_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a8ee3689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_bin = gen_root_bin(out_doc, G)\n",
    "root_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3f2efeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_base(adj, adj_tran, out_doc, G):\n",
    "    root_bin = gen_root_bin(out_doc, G)\n",
    "    \n",
    "    set1 = np.dot(root_bin, adj)\n",
    "    set2 = np.dot(root_bin, adj_tran)\n",
    "    base_bin = set1 + set2 + root_bin\n",
    "    \n",
    "    base = []\n",
    "    for x in range(len(G.nodes)):\n",
    "        if base_bin[x] > 0:\n",
    "            base.append(x)\n",
    "            \n",
    "    base = np.array(base)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "21ad23ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 15, 66, 77])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = gen_base(adj, adj_tran, out_doc, G)\n",
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "be12ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_subgraph(G, base):\n",
    "    SG = nx.subgraph(G, base)\n",
    "    A = nx.to_numpy_array(SG)\n",
    "    sub_nodes = np.array(list(SG.nodes))\n",
    "    return A, sub_nodes, SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6d4995f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 66 77 15]\n",
      "\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "A, sub_nodes, SG = gen_subgraph(G, base)\n",
    "print(sub_nodes)\n",
    "print()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa982f9",
   "metadata": {},
   "source": [
    "# 9. Principle Eigenvector Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e09f5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_mat = np.dot(A, A.T)\n",
    "a_mat = np.dot(A.T, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1c693298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_scores(K):\n",
    "    v, V = np.linalg.eig(K)\n",
    "    if len(v) == 0:\n",
    "        eig = []\n",
    "    else:\n",
    "        eig = V[:, v.argmax()]\n",
    "        eig = eig / eig.sum()\n",
    "        eig = eig.real\n",
    "        return eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "77067997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Hub pages and Scores:\n",
      "(15, 0.385)\n",
      "(77, 0.366)\n",
      "(8, 0.165)\n"
     ]
    }
   ],
   "source": [
    "eig_h = gen_scores(h_mat)\n",
    "\n",
    "if isinstance(eig_h, type(None)):\n",
    "    print('Required pages do not exist')\n",
    "\n",
    "else:\n",
    "    h_map = {}\n",
    "    for x in range(len(sub_nodes)):\n",
    "        h_map[sub_nodes[x]] = round(eig_h[x], 3)\n",
    "\n",
    "    h_map_sorted = sorted(h_map.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
    "\n",
    "    print('Top 3 Hub pages and Scores:')\n",
    "\n",
    "    count = 0\n",
    "    if len(h_map) >= 3:\n",
    "        for x in range(3):\n",
    "            print(h_map_sorted[x])\n",
    "            if x == 2:\n",
    "                count = x + 1\n",
    "                while h_map_sorted[count][1] == h_map_sorted[x][1]:\n",
    "                    print(h_map_sorted[count])\n",
    "                    count += 1\n",
    "                    if count >= len(h_map):\n",
    "                        break\n",
    "\n",
    "    else:\n",
    "        for x in range(len(h_map)):\n",
    "            print(h_map_sorted[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1f0db327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Authority pages and Scores:\n",
      "(66, 0.366)\n",
      "(8, 0.3)\n",
      "(77, 0.188)\n"
     ]
    }
   ],
   "source": [
    "eig_a = gen_scores(a_mat)\n",
    "\n",
    "if isinstance(eig_a, type(None)):\n",
    "    print('Required pages do not exist')\n",
    "    \n",
    "else:\n",
    "    a_map = {}\n",
    "    for x in range(len(sub_nodes)):\n",
    "        a_map[sub_nodes[x]] = round(eig_a[x], 3)\n",
    "\n",
    "    a_map_sorted = sorted(a_map.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
    "\n",
    "    print('Top 3 Authority pages and Scores:')\n",
    "\n",
    "    count = 0\n",
    "    if len(a_map) > 3:\n",
    "        for x in range(3):\n",
    "            print(a_map_sorted[x])\n",
    "            if x == 2:\n",
    "                count = x + 1\n",
    "                while a_map_sorted[count][1] == a_map_sorted[x][1]:\n",
    "                    print(a_map_sorted[count])\n",
    "                    count += 1\n",
    "                    if count >= len(a_map):\n",
    "                        break\n",
    "\n",
    "    else:\n",
    "        for x in range(len(a_map)):\n",
    "            print(a_map_sorted[x])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IR_Assignment_1_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
